import requests
from datetime import datetime
from bs4 import BeautifulSoup
import re


def scrape_almowafir_deals():
    """Ø³Ø­Ø¨ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ù…ÙˆÙØ± Ù…Ø¹ Ø§Ù„ØµÙˆØ±"""
    offers = []
    try:
        print("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† Ø§Ù„Ù…ÙˆÙØ±...")
        url = "https://almowafir.com/ar/coupons/"
        response = requests.get(url, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0',
            'Accept-Language': 'ar,en;q=0.9'
        })
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            coupons = soup.select('.coupon-card, .deal-card, .offer-box, [class*="coupon"], [class*="deal"]')[:20]
            
            for coupon in coupons:
                discount_el = coupon.select_one('[class*="discount"], [class*="percent"], .badge, .off')
                title_el = coupon.select_one('h3, h4, .title, .description, p')
                link_el = coupon.select_one('a[href*="coupon"], a[href*="deal"], a.btn')
                store_el = coupon.select_one('.store-name, .brand, img[alt]')
                image_el = coupon.select_one('img[src]')
                
                discount = ""
                if discount_el:
                    discount = discount_el.get_text(strip=True)
                
                all_text = coupon.get_text()
                percent_match = re.search(r'(\d+)\s*%', all_text)
                if percent_match:
                    discount = f"{percent_match.group(1)}%"
                
                if discount and '%' in discount:
                    store = ""
                    if store_el:
                        store = store_el.get('alt', '') or store_el.get_text(strip=True)
                    
                    title = f"Ø®ØµÙ… {discount}"
                    if store:
                        title = f"Ø®ØµÙ… {discount} Ù…Ù† {store}"
                    
                    link = ""
                    if link_el:
                        link = link_el.get('href', '')
                    
                    image_url = ""
                    if image_el:
                        image_url = image_el.get('src', '')
                        if image_url and not image_url.startswith('http'):
                             image_url = f"https://almowafir.com{image_url}"

                    if title:
                        offers.append({
                            'title': clean_title(title),
                            'link': link if link.startswith('http') else f"https://almowafir.com{link}",
                            'price': discount,
                            'category': 'ÙƒÙˆØ¨ÙˆÙ†Ø§Øª',
                            'source': 'Ø§Ù„Ù…ÙˆÙØ±',
                            'image_url': image_url,
                            'description': f"ÙƒÙˆØ¨ÙˆÙ† Ø®ØµÙ… {discount} ÙØ¹Ø§Ù„ Ø¹Ù„Ù‰ {store}. Ø§Ù†Ø³Ø® Ø§Ù„ÙƒÙˆØ¯ ÙˆØ§Ø³ØªØ®Ø¯Ù…Ù‡ Ø¹Ù†Ø¯ Ø§Ù„Ø¯ÙØ¹.",
                            'date': datetime.now().isoformat()
                        })
            print(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(offers)} Ø¹Ø±Ø¶ Ù…Ù† Ø§Ù„Ù…ÙˆÙØ±")
    except Exception as e:
        print(f"Ø®Ø·Ø£ Ø§Ù„Ù…ÙˆÙØ±: {e}")
    return offers


def scrape_noon_deals():
    """Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ù…Ø¹ Ø§Ù„ØµÙˆØ±"""
    offers = []
    try:
        print("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† Ù†ÙˆÙ†...")
        url = "https://www.noon.com/saudi-ar/offers/"
        response = requests.get(url, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'
        })
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            products = soup.select('[class*="product"], [class*="item"], article')[:15]
            
            for prod in products:
                title_el = prod.select_one('[class*="title"], [class*="name"], h3, h4')
                price_el = prod.select_one('[class*="price"], [class*="now"]')
                old_price = prod.select_one('[class*="was"], [class*="old"], del, s')
                link_el = prod.select_one('a[href]')
                image_el = prod.select_one('img[src]')
                
                if title_el and old_price:
                    title = clean_title(title_el.get_text(strip=True))
                    price = price_el.get_text(strip=True) if price_el else ""
                    link = link_el.get('href', '') if link_el else ""
                    image_url = ""
                    if image_el:
                         image_url = image_el.get('src', '')
                    
                    if title and len(title) > 5:
                        offers.append({
                            'title': f"Ø¹Ø±Ø¶ Ù†ÙˆÙ†: {title[:60]}",
                            'link': link if link.startswith('http') else f"https://noon.com{link}",
                            'price': price,
                            'category': 'ØªØ®ÙÙŠØ¶Ø§Øª',
                            'source': 'Ù†ÙˆÙ†',
                            'image_url': image_url,
                            'description': f"Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ {title} Ø¨Ø³Ø¹Ø± {price} ÙÙ‚Ø·! (Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø³Ø§Ø¨Ù‚: {old_price.get_text(strip=True)})",
                            'date': datetime.now().isoformat()
                        })
            print(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(offers)} Ù…Ù† Ù†ÙˆÙ†")
    except Exception as e:
        print(f"Ø®Ø·Ø£ Ù†ÙˆÙ†: {e}")
    return offers


def scrape_extra_deals():
    """Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ø§ÙƒØ³ØªØ±Ø§ Ù…Ø¹ Ø§Ù„ØµÙˆØ±"""
    offers = []
    try:
        print("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† Ø§ÙƒØ³ØªØ±Ø§...")
        url = "https://www.extra.com/ar-sa/offers"
        response = requests.get(url, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 Chrome/120.0.0.0'
        })
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            products = soup.select('.product, .item, article, [class*="product"]')[:15]
            
            for prod in products:
                title_el = prod.select_one('.title, .name, h3, h4, a[title]')
                price_el = prod.select_one('.price, [class*="price"]')
                link_el = prod.select_one('a[href]')
                image_el = prod.select_one('img[src]')
                
                if title_el:
                    title = title_el.get('title') or title_el.get_text(strip=True)
                    title = clean_title(title)
                    price = price_el.get_text(strip=True) if price_el else ""
                    link = link_el.get('href', '') if link_el else ""
                    image_url = ""
                    if image_el:
                        image_url = image_el.get('src', '')
                        if image_url and not image_url.startswith('http'):
                            image_url = f"https://www.extra.com{image_url}"

                    if title and len(title) > 5:
                        offers.append({
                            'title': f"Ø¹Ø±Ø¶ Ø§ÙƒØ³ØªØ±Ø§: {title[:60]}",
                            'link': link if link.startswith('http') else f"https://extra.com{link}",
                            'price': price,
                            'category': 'Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª',
                            'source': 'Ø§ÙƒØ³ØªØ±Ø§',
                            'image_url': image_url,
                            'description': f"Ø¹Ø±Ø¶ Ø®Ø§Øµ Ù…Ù† Ø§ÙƒØ³ØªØ±Ø§ Ø¹Ù„Ù‰ {title}. Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ: {price}",
                            'date': datetime.now().isoformat()
                        })
            print(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(offers)} Ù…Ù† Ø§ÙƒØ³ØªØ±Ø§")
    except Exception as e:
        print(f"Ø®Ø·Ø£ Ø§ÙƒØ³ØªØ±Ø§: {e}")
    return offers

def scrape_cobone_deals():
    """Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… Ù…Ù† ÙƒÙˆØ¨ÙˆÙ†"""
    offers = []
    try:
        print("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† ÙƒÙˆØ¨ÙˆÙ† (Ù…Ø·Ø§Ø¹Ù…)...")
        # Ù†Ø³Ø­Ø¨ Ù…Ù† Ø§Ù„Ø±ÙŠØ§Ø¶ ÙˆØ¬Ø¯Ø©
        urls = [
            "https://www.cobone.com/ar/deals/riyadh/food-dining",
            "https://www.cobone.com/ar/deals/jeddah/food-dining"
        ]
        
        for url in urls:
            response = requests.get(url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                deals = soup.select('.deal-box, .deal_item')[:10]
                
                for deal in deals:
                    title_el = deal.select_one('.title, h3, h2')
                    price_el = deal.select_one('.price, .actual-price')
                    img_el = deal.select_one('img')
                    link_el = deal.select_one('a')
                    
                    if title_el and link_el:
                        title = clean_title(title_el.get_text(strip=True))
                        price = price_el.get_text(strip=True) if price_el else "Ø®ØµÙ… Ø®Ø§Øµ"
                        link = link_el.get('href')
                        if link and not link.startswith('http'):
                            link = f"https://www.cobone.com{link}"
                            
                        image_url = img_el.get('data-original') or img_el.get('src') if img_el else ""
                        
                        offers.append({
                            'title': title,
                            'link': link,
                            'price': price,
                            'category': 'Ù…Ø·Ø§Ø¹Ù…',
                            'source': 'ÙƒÙˆØ¨ÙˆÙ†',
                            'image_url': image_url,
                            'description': f"Ø¹Ø±Ø¶ Ù…Ø·Ø§Ø¹Ù… Ù…Ù…ÙŠØ²: {title} Ø¨Ø³Ø¹Ø± {price}",
                            'date': datetime.now().isoformat()
                        })
        print(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(offers)} Ø¹Ø±Ø¶ Ù…Ø·Ø§Ø¹Ù…")
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙƒÙˆØ¨ÙˆÙ†: {e}")
    return offers


def scrape_ilofo_deals():
    """Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… ÙˆØ§Ù„Ù‚Ù‡ÙˆØ© (ÙˆØ§Ù„Ø¨Ù†ÙˆÙƒ) Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø¹Ø±ÙˆØ¶ (ilofo)"""
    offers = []
    try:
        print("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† Ø¹Ø±ÙˆØ¶ (ilofo)...")
        # ØµÙØ­Ø© Ø§Ù„Ù…Ø·Ø§Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§Ù‡ÙŠ (ØºØ§Ù„Ø¨Ø§Ù‹ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 1+1 ÙˆØ¹Ø±ÙˆØ¶ Ø§Ù„Ù‚Ù‡ÙˆØ©)
        url = "https://www.ilofo.com/saudi/offers/restaurants"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # Select offer blocks
            offer_blocks = soup.select('.col-md-3, .offer-box, .card')[:15]
            
            for block in offer_blocks:
                title_el = block.select_one('.card-title, h5, h4, a[title]')
                img_el = block.select_one('img')
                link_el = block.select_one('a')
                
                if title_el and img_el:
                    title = clean_title(title_el.get_text(strip=True))
                    image_url = img_el.get('src') or img_el.get('data-src')
                    if image_url and not image_url.startswith('http'):
                        image_url = f"https://www.ilofo.com{image_url}"
                        
                    # Filter for keywords: Bank, Free, 1+1, Coffee
                    keywords = ['Ù…Ø¬Ø§Ù†Ø§', '1+1', 'Ø¨Ù†Ùƒ', 'Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠ', 'Ù‚Ù‡ÙˆØ©', 'riyal', 'Ø±ÙŠØ§Ù„']
                    # We take mostly everything from here as it's targeted flyers, but checking keywords helps prioritization
                    # For now, take all restaurant offers found
                    
                    details_link = link_el.get('href') if link_el else ""
                    if details_link and not details_link.startswith('http'):
                        details_link = f"https://www.ilofo.com{details_link}"
                        
                    offers.append({
                        'title': title,
                        'link': details_link or url,
                        'price': "Ø¹Ø±Ø¶ Ù†Ø´Ø±Ø©", # Flyers often have multiple prices
                        'category': 'Ù…Ø·Ø§Ø¹Ù…/Ø¨Ù†ÙˆÙƒ',
                        'source': 'ilofo',
                        'image_url': image_url,
                        'description': f"Ø´Ø§Ù‡Ø¯ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¹Ø±Ø¶: {title}. Ù‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø±ÙˆØ¶ 1+1 Ø£Ùˆ Ø®ØµÙˆÙ…Ø§Øª Ø¨Ù†ÙƒÙŠØ©.",
                        'date': datetime.now().isoformat()
                    })
                    
        print(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(offers)} Ø¹Ø±Ø¶ Ù…Ù† ilofo")
    except Exception as e:
        print(f"Ø®Ø·Ø£ ilofo: {e}")
    return offers


def clean_title(title: str) -> str:
    if not title:
        return ""
    title = re.sub(r'<[^>]+>', '', title)
    title = title.replace('*', '').replace('_', '').replace('[', '').replace(']', '')
    title = ' '.join(title.split())
    return title[:100] if title else ""


def extract_price(text: str) -> str:
    if not text:
        return ""
    patterns = [r'\d+%', r'\d+\s*(?:Ø±ÙŠØ§Ù„|Ø±\.Ø³|SAR)']
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group()
    return ""


def fetch_rss_offers(feed_url: str, feed_name: str, category: str):
    return []


def fetch_all_rss_feeds(feeds: list):
    """Ø³Ø­Ø¨ ÙƒÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶"""
    all_offers = []
    
    print("=" * 40)
    print("ğŸ” Ø¨Ø¯Ø¡ Ø³Ø­Ø¨ Ø§Ù„Ø¹Ø±ÙˆØ¶ (Ù…Ø¹ Ø§Ù„ØµÙˆØ±)...")
    print("=" * 40)
    
    try:
        all_offers.extend(scrape_almowafir_deals())
    except: pass
    
    try:
        all_offers.extend(scrape_noon_deals())
    except: pass
    
    try:
        all_offers.extend(scrape_extra_deals())
    except: pass

    try:
        all_offers.extend(scrape_cobone_deals())
    except: pass

    try:
        all_offers.extend(scrape_delivery_apps())
    except: pass

    try:
        all_offers.extend(scrape_ilofo_deals())
    except: pass
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„ (Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¸Ù‡ÙˆØ± Ø´ÙŠØ¡ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…)
    # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ø¹Ø±ÙˆØ¶ØŒ Ù„Ø§ Ù†Ø±Ø³Ù„ Ø¹Ø±ÙˆØ¶ ØªØ¬Ø±ÙŠØ¨ÙŠØ© (Ø§Ù„ØµØ¯Ù‚ Ø£Ù‡Ù…)
    if not all_offers:
        print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ø±ÙˆØ¶ Ø¬Ø¯ÙŠØ¯Ø© Ø­Ø§Ù„ÙŠØ§Ù‹.")
    
    print("=" * 40)
    print(f"âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ø±ÙˆØ¶: {len(all_offers)}")
    print("=" * 40)
    
    return all_offers


def fetch_webpage_offers(url: str, selectors: dict):
    return []
