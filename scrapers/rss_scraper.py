import requests
from datetime import datetime
from bs4 import BeautifulSoup
import re


def clean_title(title: str) -> str:
    if not title:
        return ""
    title = re.sub(r'<[^>]+>', '', title)
    title = title.replace('*', '').replace('_', '').replace('[', '').replace(']', '')
    title = ' '.join(title.split())
    return title[:100] if title else ""


# ============================================
# 1. Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… ÙˆØ§Ù„ÙƒÙˆÙÙŠÙ‡Ø§Øª
# ============================================

def scrape_restaurant_offers():
    """Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… ÙˆØ§Ù„ÙƒÙˆÙÙŠÙ‡Ø§Øª"""
    offers = []
    print("ğŸ” Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…Ø·Ø§Ø¹Ù…...")
    
    # Ù…ØµØ¯Ø± 1: ÙƒÙˆØ¨ÙˆÙ†
    try:
        urls = [
            "https://www.cobone.com/ar/deals/riyadh/food-dining",
            "https://www.cobone.com/ar/deals/jeddah/food-dining"
        ]
        for url in urls:
            response = requests.get(url, timeout=20, headers={'User-Agent': 'Mozilla/5.0'})
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                deals = soup.select('.deal-box, .deal_item, .card')[:8]
                for deal in deals:
                    title_el = deal.select_one('.title, h3, h2, h4')
                    img_el = deal.select_one('img')
                    link_el = deal.select_one('a')
                    if title_el:
                        offers.append({
                            'title': clean_title(title_el.get_text(strip=True)),
                            'link': link_el.get('href', '') if link_el else url,
                            'price': 'Ø¹Ø±Ø¶ Ù…Ø·Ø§Ø¹Ù…',
                            'category': 'Ù…Ø·Ø§Ø¹Ù…',
                            'source': 'ÙƒÙˆØ¨ÙˆÙ†',
                            'image_url': img_el.get('src', '') if img_el else '',
                            'description': 'Ø¹Ø±Ø¶ Ù…Ø·Ø§Ø¹Ù… Ù…Ù…ÙŠØ² Ù…Ù† ÙƒÙˆØ¨ÙˆÙ†',
                            'date': datetime.now().isoformat()
                        })
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙƒÙˆØ¨ÙˆÙ†: {e}")
    
    print(f"âœ… Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…Ø·Ø§Ø¹Ù…: {len(offers)}")
    return offers


# ============================================
# 2. Ø¹Ø±ÙˆØ¶ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙˆØµÙŠÙ„
# ============================================

def scrape_delivery_apps():
    """ÙƒÙˆØ¨ÙˆÙ†Ø§Øª ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙˆØµÙŠÙ„"""
    offers = []
    print("ğŸ›µ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ ÙƒÙˆØ¨ÙˆÙ†Ø§Øª Ø§Ù„ØªÙˆØµÙŠÙ„...")
    
    apps = [
        ("Ù‡Ù†Ù‚Ø±Ø³ØªÙŠØ´Ù†", "hungerstation"),
        ("ØªÙˆÙŠÙˆ", "toyou"),
        ("Ø¬Ø§Ù‡Ø²", "jahez"),
        ("Ù…Ø±Ø³ÙˆÙ„", "mrsool"),
        ("Ù†ÙˆÙ† ÙÙˆØ¯", "noon-food"),
        ("Ø·Ù„Ø¨Ø§Øª", "talabat"),
    ]
    
    for app_name, app_slug in apps:
        try:
            url = f"https://almowafir.com/ar/stores/{app_slug}/"
            resp = requests.get(url, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ ÙƒÙˆØ¨ÙˆÙ†
                codes = soup.select('[class*="coupon"], [class*="code"], .offer-card')[:2]
                for code in codes:
                    text = code.get_text(strip=True)[:80]
                    offers.append({
                        'title': f'ÙƒÙˆØ¨ÙˆÙ† {app_name}: {text}',
                        'link': url,
                        'price': 'ÙƒÙˆØ¯ Ø®ØµÙ…',
                        'category': 'ØªÙˆØµÙŠÙ„',
                        'source': app_name,
                        'image_url': '',
                        'description': f'Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¨ÙˆÙ† Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø®ØµÙ… ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ {app_name}',
                        'date': datetime.now().isoformat()
                    })
        except:
            continue
    
    print(f"âœ… ÙƒÙˆØ¨ÙˆÙ†Ø§Øª Ø§Ù„ØªÙˆØµÙŠÙ„: {len(offers)}")
    return offers


# ============================================
# 3. Ø¹Ø±ÙˆØ¶ Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø¨Ù†ÙˆÙƒ
# ============================================

def scrape_bank_offers():
    """Ø¹Ø±ÙˆØ¶ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø¨Ù†ÙƒÙŠØ©"""
    offers = []
    print("ğŸ’³ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ø§Ù„Ø¨Ù†ÙˆÙƒ...")
    
    banks = [
        ("Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠ", "alrajhi-bank"),
        ("Ø§Ù„Ø£Ù‡Ù„ÙŠ", "ncb"),
        ("Ø§Ù„Ø¥Ù†Ù…Ø§Ø¡", "alinma-bank"),
        ("STC Pay", "stc-pay"),
    ]
    
    for bank_name, bank_slug in banks:
        try:
            url = f"https://almowafir.com/ar/stores/{bank_slug}/"
            resp = requests.get(url, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                deals = soup.select('[class*="offer"], [class*="deal"], .card')[:2]
                for deal in deals:
                    text = deal.get_text(strip=True)[:80]
                    if text:
                        offers.append({
                            'title': f'Ø¹Ø±Ø¶ {bank_name}: {text}',
                            'link': url,
                            'price': 'ÙƒØ§Ø´ Ø¨Ø§Ùƒ',
                            'category': 'Ø¨Ù†ÙˆÙƒ',
                            'source': bank_name,
                            'image_url': '',
                            'description': f'Ø¹Ø±Ø¶ Ø®Ø§Øµ Ù„Ø­Ø§Ù…Ù„ÙŠ Ø¨Ø·Ø§Ù‚Ø§Øª {bank_name}',
                            'date': datetime.now().isoformat()
                        })
        except:
            continue
    
    print(f"âœ… Ø¹Ø±ÙˆØ¶ Ø§Ù„Ø¨Ù†ÙˆÙƒ: {len(offers)}")
    return offers


# ============================================
# 4. Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (Ø£Ù…Ø§Ø²ÙˆÙ†ØŒ Ù†ÙˆÙ†ØŒ Ø¹Ù„ÙŠ Ø§ÙƒØ³Ø¨Ø±Ø³)
# ============================================

def scrape_global_sites():
    """Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©"""
    offers = []
    print("ğŸŒ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©...")
    
    sites = [
        ("Ø£Ù…Ø§Ø²ÙˆÙ†", "amazon-sa", "https://almowafir.com/ar/stores/amazon-sa/"),
        ("Ù†ÙˆÙ†", "noon", "https://almowafir.com/ar/stores/noon/"),
        ("Ø¹Ù„ÙŠ Ø§ÙƒØ³Ø¨Ø±Ø³", "aliexpress", "https://almowafir.com/ar/stores/aliexpress/"),
        ("Ø´ÙŠ Ø¥Ù†", "shein", "https://almowafir.com/ar/stores/shein/"),
        ("Ù†Ù…Ø´ÙŠ", "namshi", "https://almowafir.com/ar/stores/namshi/"),
    ]
    
    for site_name, site_slug, url in sites:
        try:
            resp = requests.get(url, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙˆØ¨ÙˆÙ†Ø§Øª ÙˆØ§Ù„Ø¹Ø±ÙˆØ¶
                items = soup.select('[class*="coupon"], [class*="offer"], [class*="deal"]')[:3]
                for item in items:
                    text = item.get_text(strip=True)[:80]
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ø³Ø¨Ø© Ø§Ù„Ø®ØµÙ…
                    percent = re.search(r'(\d+)\s*%', text)
                    price = f"{percent.group(1)}%" if percent else "Ø®ØµÙ…"
                    
                    if text and len(text) > 5:
                        offers.append({
                            'title': f'{site_name}: {text}',
                            'link': url,
                            'price': price,
                            'category': 'ØªØ³ÙˆÙ‚',
                            'source': site_name,
                            'image_url': '',
                            'description': f'ÙƒÙˆØ¨ÙˆÙ† Ø®ØµÙ… ÙØ¹Ø§Ù„ Ø¹Ù„Ù‰ Ù…ÙˆÙ‚Ø¹ {site_name}',
                            'date': datetime.now().isoformat()
                        })
        except:
            continue
    
    print(f"âœ… Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©: {len(offers)}")
    return offers


# ============================================
# 5. Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ÙˆÙØ± Ø§Ù„Ø¹Ø§Ù…Ø©
# ============================================

def scrape_almowafir_deals():
    """Ø£ÙØ¶Ù„ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ù…Ù† Ø§Ù„Ù…ÙˆÙØ±"""
    offers = []
    print("ğŸ·ï¸ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ÙˆÙØ±...")
    
    try:
        url = "https://almowafir.com/ar/coupons/"
        resp = requests.get(url, timeout=20, headers={
            'User-Agent': 'Mozilla/5.0',
            'Accept-Language': 'ar'
        })
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            items = soup.select('[class*="coupon"], [class*="deal"], .card')[:10]
            for item in items:
                title = item.get_text(strip=True)[:80]
                link = item.select_one('a')
                img = item.select_one('img')
                percent = re.search(r'(\d+)\s*%', title)
                
                if title and len(title) > 10:
                    offers.append({
                        'title': clean_title(title),
                        'link': link.get('href', '') if link else url,
                        'price': f"{percent.group(1)}%" if percent else "Ø®ØµÙ…",
                        'category': 'ÙƒÙˆØ¨ÙˆÙ†Ø§Øª',
                        'source': 'Ø§Ù„Ù…ÙˆÙØ±',
                        'image_url': img.get('src', '') if img else '',
                        'description': 'ÙƒÙˆØ¨ÙˆÙ† Ø®ØµÙ… ÙØ¹Ø§Ù„ Ù…Ù† Ø§Ù„Ù…ÙˆÙØ±',
                        'date': datetime.now().isoformat()
                    })
    except Exception as e:
        print(f"Ø®Ø·Ø£ Ø§Ù„Ù…ÙˆÙØ±: {e}")
    
    print(f"âœ… Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ÙˆÙØ±: {len(offers)}")
    return offers


# ============================================
# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ============================================

def fetch_all_rss_feeds(feeds: list):
    """Ø³Ø­Ø¨ ÙƒÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶"""
    all_offers = []
    
    print("=" * 50)
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø³Ø­Ø¨ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„Ø´Ø§Ù…Ù„Ø©...")
    print("=" * 50)
    
    # 1. Ù…Ø·Ø§Ø¹Ù… ÙˆÙƒÙˆÙÙŠÙ‡Ø§Øª
    try:
        all_offers.extend(scrape_restaurant_offers())
    except: pass
    
    # 2. ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙˆØµÙŠÙ„
    try:
        all_offers.extend(scrape_delivery_apps())
    except: pass
    
    # 3. Ø¹Ø±ÙˆØ¶ Ø§Ù„Ø¨Ù†ÙˆÙƒ
    try:
        all_offers.extend(scrape_bank_offers())
    except: pass
    
    # 4. Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©
    try:
        all_offers.extend(scrape_global_sites())
    except: pass
    
    # 5. Ø§Ù„Ù…ÙˆÙØ±
    try:
        all_offers.extend(scrape_almowafir_deals())
    except: pass
    
    print("=" * 50)
    print(f"âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ø±ÙˆØ¶: {len(all_offers)}")
    print("=" * 50)
    
    return all_offers


# Ø¯ÙˆØ§Ù„ Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„ØªÙˆØ§ÙÙ‚
def fetch_rss_offers(feed_url: str, feed_name: str, category: str):
    return []

def fetch_webpage_offers(url: str, selectors: dict):
    return []
