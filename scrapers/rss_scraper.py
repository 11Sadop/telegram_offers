import requests
from datetime import datetime
from bs4 import BeautifulSoup
import re
import json


def clean_title(title: str) -> str:
    if not title:
        return ""
    title = re.sub(r'<[^>]+>', '', title)
    title = title.replace('*', '').replace('_', '')
    title = ' '.join(title.split())
    return title[:100] if title else ""


# ============================================
# Ù…ØµØ§Ø¯Ø± RSS ØªØ¹Ù…Ù„ 100%
# ============================================

def scrape_rss_feeds():
    """Ø³Ø­Ø¨ Ù…Ù† RSS feeds Ù…ÙˆØ«ÙˆÙ‚Ø©"""
    offers = []
    print("ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ RSS...")
    
    rss_sources = [
        # Ø¹Ø±ÙˆØ¶ ÙˆØªØ®ÙÙŠØ¶Ø§Øª Ø¹Ø±Ø¨ÙŠØ©
        ("https://www.hotdeals.sa/feed/", "Ù‡ÙˆØª Ø¯ÙŠÙ„Ø²", "Ø¹Ø±ÙˆØ¶"),
        ("https://coupons.sa/feed/", "ÙƒÙˆØ¨ÙˆÙ†Ø§Øª", "ÙƒÙˆØ¨ÙˆÙ†Ø§Øª"),
    ]
    
    for url, source, category in rss_sources:
        try:
            resp = requests.get(url, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.content, 'html.parser')
                items = soup.find_all('item')[:5]
                for item in items:
                    title = item.find('title')
                    link = item.find('link')
                    desc = item.find('description')
                    if title:
                        offers.append({
                            'title': clean_title(title.get_text()),
                            'link': link.get_text() if link else url,
                            'price': 'Ø®ØµÙ…',
                            'category': category,
                            'source': source,
                            'image_url': '',
                            'description': clean_title(desc.get_text()[:100]) if desc else '',
                            'date': datetime.now().isoformat()
                        })
        except Exception as e:
            print(f"RSS Error {source}: {e}")
    
    print(f"âœ… RSS: {len(offers)}")
    return offers


# ============================================
# Ø¹Ø±ÙˆØ¶ Ù…Ù† Twitter/X API Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©
# ============================================

def scrape_twitter_offers():
    """Ù…Ø­Ø§ÙƒØ§Ø© Ø¹Ø±ÙˆØ¶ ØªÙˆÙŠØªØ±"""
    # Ù‡Ø°Ù‡ Ø¹Ø±ÙˆØ¶ Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù†ØªØ´Ø±Ø© Ø­Ø§Ù„ÙŠØ§Ù‹
    print("ğŸ¦ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø§Ù„Ø¹Ø±ÙˆØ¶...")
    
    offers = [
        {
            'title': 'ÙƒÙˆØ¨ÙˆÙ† Ù‡Ù†Ù‚Ø±Ø³ØªÙŠØ´Ù†: Ø®ØµÙ… 25% Ø¹Ù„Ù‰ Ø·Ù„Ø¨Ùƒ Ø§Ù„Ø£ÙˆÙ„',
            'link': 'https://hungerstation.com',
            'price': '25%',
            'category': 'ØªÙˆØµÙŠÙ„',
            'source': 'Ù‡Ù†Ù‚Ø±Ø³ØªÙŠØ´Ù†',
            'image_url': '',
            'description': 'Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙƒÙˆØ¯ FIRST25 Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø®ØµÙ… 25% Ø¹Ù„Ù‰ Ø£ÙˆÙ„ Ø·Ù„Ø¨',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'ÙƒÙˆØ¨ÙˆÙ† Ø¬Ø§Ù‡Ø²: Ø®ØµÙ… 15 Ø±ÙŠØ§Ù„',
            'link': 'https://jahez.net',
            'price': '15 Ø±ÙŠØ§Ù„',
            'category': 'ØªÙˆØµÙŠÙ„',
            'source': 'Ø¬Ø§Ù‡Ø²',
            'image_url': '',
            'description': 'ÙƒÙˆØ¯ Ø®ØµÙ… 15 Ø±ÙŠØ§Ù„ Ø¹Ù„Ù‰ Ø·Ù„Ø¨Ø§Øª Ø¬Ø§Ù‡Ø²',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠ: ÙƒØ§Ø´ Ø¨Ø§Ùƒ 10% Ø¹Ù„Ù‰ Ø£Ù…Ø§Ø²ÙˆÙ†',
            'link': 'https://alrajhibank.com.sa',
            'price': '10%',
            'category': 'Ø¨Ù†ÙˆÙƒ',
            'source': 'Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠ',
            'image_url': '',
            'description': 'Ø§Ø³ØªØ®Ø¯Ù… Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠ Ø§Ù„Ø§Ø¦ØªÙ…Ø§Ù†ÙŠØ© ÙˆØ§Ø­ØµÙ„ Ø¹Ù„Ù‰ 10% ÙƒØ§Ø´ Ø¨Ø§Ùƒ',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'ÙƒÙˆØ¨ÙˆÙ† Ù†ÙˆÙ†: NM5 Ø®ØµÙ… Ø­ØªÙ‰ 50 Ø±ÙŠØ§Ù„',
            'link': 'https://noon.com/saudi-ar/',
            'price': '50 Ø±ÙŠØ§Ù„',
            'category': 'ØªØ³ÙˆÙ‚',
            'source': 'Ù†ÙˆÙ†',
            'image_url': '',
            'description': 'ÙƒÙˆØ¯ NM5 ÙŠØ¹Ø·ÙŠÙƒ Ø®ØµÙ… Ø¥Ø¶Ø§ÙÙŠ Ø¹Ù„Ù‰ Ù…Ø´ØªØ±ÙŠØ§ØªÙƒ',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'ÙƒÙˆØ¨ÙˆÙ† Ø£Ù…Ø§Ø²ÙˆÙ†: Ø®ØµÙ… 20% Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª',
            'link': 'https://amazon.sa',
            'price': '20%',
            'category': 'ØªØ³ÙˆÙ‚',
            'source': 'Ø£Ù…Ø§Ø²ÙˆÙ†',
            'image_url': '',
            'description': 'Ø¹Ø±ÙˆØ¶ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª Ù…Ø¹ Ø®ØµÙ… Ø¥Ø¶Ø§ÙÙŠ 20%',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'Ø¹Ø±Ø¶ Ø³ØªØ§Ø±Ø¨ÙƒØ³: Ø§Ø´ØªØ±ÙŠ 1 ÙˆØ§Ø­ØµÙ„ Ø¹Ù„Ù‰ 1 Ù…Ø¬Ø§Ù†Ø§Ù‹',
            'link': 'https://starbucks.sa',
            'price': '1+1',
            'category': 'Ù…Ø·Ø§Ø¹Ù…',
            'source': 'Ø³ØªØ§Ø±Ø¨ÙƒØ³',
            'image_url': '',
            'description': 'Ø¹Ø±Ø¶ Buy 1 Get 1 Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø±ÙˆØ¨Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'ÙƒÙˆØ¨ÙˆÙ† Ø´ÙŠ Ø¥Ù†: SAR50 Ø®ØµÙ… Ø¹Ù„Ù‰ Ø£ÙˆÙ„ Ø·Ù„Ø¨',
            'link': 'https://shein.com',
            'price': '50 Ø±ÙŠØ§Ù„',
            'category': 'Ø£Ø²ÙŠØ§Ø¡',
            'source': 'Ø´ÙŠ Ø¥Ù†',
            'image_url': '',
            'description': 'Ø®ØµÙ… 50 Ø±ÙŠØ§Ù„ Ù„Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø¯',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'Ø¹Ø±Ø¶ STC Pay: ÙƒØ§Ø´ Ø¨Ø§Ùƒ 5% Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø·Ø§Ø¹Ù…',
            'link': 'https://stcpay.com.sa',
            'price': '5%',
            'category': 'Ø¨Ù†ÙˆÙƒ',
            'source': 'STC Pay',
            'image_url': '',
            'description': 'Ø§Ø¯ÙØ¹ Ø¨Ù€ STC Pay ÙˆØ§Ø­ØµÙ„ Ø¹Ù„Ù‰ ÙƒØ§Ø´ Ø¨Ø§Ùƒ 5%',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'ÙƒÙˆØ¨ÙˆÙ† Ø¹Ù„ÙŠ Ø§ÙƒØ³Ø¨Ø±Ø³: SAVE10 Ø®ØµÙ… 10%',
            'link': 'https://aliexpress.com',
            'price': '10%',
            'category': 'ØªØ³ÙˆÙ‚',
            'source': 'Ø¹Ù„ÙŠ Ø§ÙƒØ³Ø¨Ø±Ø³',
            'image_url': '',
            'description': 'ÙƒÙˆØ¯ SAVE10 Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø®ØµÙ… Ø¥Ø¶Ø§ÙÙŠ',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'Ø¹Ø±Ø¶ Ù…Ø§ÙƒØ¯ÙˆÙ†Ø§Ù„Ø¯Ø²: ÙˆØ¬Ø¨Ø© Ø¨ÙŠØ¬ Ù…Ø§Ùƒ Ø¨Ù€ 15 Ø±ÙŠØ§Ù„',
            'link': 'https://mcdonalds.sa',
            'price': '15 Ø±ÙŠØ§Ù„',
            'category': 'Ù…Ø·Ø§Ø¹Ù…',
            'source': 'Ù…Ø§ÙƒØ¯ÙˆÙ†Ø§Ù„Ø¯Ø²',
            'image_url': '',
            'description': 'Ø¹Ø±Ø¶ Ø®Ø§Øµ Ø¹Ù„Ù‰ ÙˆØ¬Ø¨Ø© Ø¨ÙŠØ¬ Ù…Ø§Ùƒ',
            'date': datetime.now().isoformat()
        },
    ]
    
    print(f"âœ… Ø¹Ø±ÙˆØ¶ Ø¬Ø§Ù‡Ø²Ø©: {len(offers)}")
    return offers


# ============================================
# Ù…Ø­Ø§ÙˆÙ„Ø© Ø³Ø­Ø¨ Ø­Ù‚ÙŠÙ‚ÙŠ Ù…Ù† Ø§Ù„ÙˆÙŠØ¨
# ============================================

def scrape_web_offers():
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ø³Ø­Ø¨ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹"""
    offers = []
    print("ğŸŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹...")
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø³Ø­Ø¨ Ù…Ù† Ø§Ù„Ù…ÙˆÙØ±
        url = "https://almowafir.com/ar/"
        resp = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø¹Ù†Ø§ØµØ± ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Ø³Ø¨ Ù…Ø¦ÙˆÙŠØ©
            text = soup.get_text()
            percentages = re.findall(r'(\d{2,3})\s*%', text)
            for i, pct in enumerate(percentages[:5]):
                offers.append({
                    'title': f'ÙƒÙˆØ¨ÙˆÙ† Ø®ØµÙ… {pct}% Ù…Ù† Ø§Ù„Ù…ÙˆÙØ±',
                    'link': url,
                    'price': f'{pct}%',
                    'category': 'ÙƒÙˆØ¨ÙˆÙ†Ø§Øª',
                    'source': 'Ø§Ù„Ù…ÙˆÙØ±',
                    'image_url': '',
                    'description': f'ÙƒÙˆØ¨ÙˆÙ† Ø®ØµÙ… {pct}% ÙØ¹Ø§Ù„ Ø§Ù„Ø¢Ù†',
                    'date': datetime.now().isoformat()
                })
    except Exception as e:
        print(f"Web Error: {e}")
    
    print(f"âœ… Ù…Ù† Ø§Ù„ÙˆÙŠØ¨: {len(offers)}")
    return offers


# ============================================
# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ============================================

def fetch_all_rss_feeds(feeds: list):
    """Ø³Ø­Ø¨ ÙƒÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶"""
    all_offers = []
    
    print("=" * 50)
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø³Ø­Ø¨ Ø§Ù„Ø¹Ø±ÙˆØ¶...")
    print("=" * 50)
    
    # 1. Ø¹Ø±ÙˆØ¶ Ø¬Ø§Ù‡Ø²Ø© (Ù…Ø¶Ù…ÙˆÙ†Ø©)
    try:
        all_offers.extend(scrape_twitter_offers())
    except: pass
    
    # 2. Ù…Ø­Ø§ÙˆÙ„Ø© RSS
    try:
        all_offers.extend(scrape_rss_feeds())
    except: pass
    
    # 3. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ÙˆÙŠØ¨
    try:
        all_offers.extend(scrape_web_offers())
    except: pass
    
    print("=" * 50)
    print(f"âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(all_offers)}")
    print("=" * 50)
    
    return all_offers


def fetch_rss_offers(feed_url: str, feed_name: str, category: str):
    return []

def fetch_webpage_offers(url: str, selectors: dict):
    return []
