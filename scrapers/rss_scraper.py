import requests
from datetime import datetime
from bs4 import BeautifulSoup
import re


def scrape_almowafir_deals():
    """Ø³Ø­Ø¨ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ù…ÙˆÙØ±"""
    offers = []
    try:
        print("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† Ø§Ù„Ù…ÙˆÙØ±...")
        # ØµÙØ­Ø© Ø§Ù„Ø¹Ø±ÙˆØ¶ ÙˆØ§Ù„ÙƒÙˆØ¨ÙˆÙ†Ø§Øª
        url = "https://almowafir.com/ar/coupons/"
        response = requests.get(url, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0',
            'Accept-Language': 'ar,en;q=0.9'
        })
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙˆØ¨ÙˆÙ†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©
            coupons = soup.select('.coupon-card, .deal-card, .offer-box, [class*="coupon"], [class*="deal"]')[:20]
            
            for coupon in coupons:
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Øµ Ø§Ù„Ø®ØµÙ…
                discount_el = coupon.select_one('[class*="discount"], [class*="percent"], .badge, .off')
                title_el = coupon.select_one('h3, h4, .title, .description, p')
                link_el = coupon.select_one('a[href*="coupon"], a[href*="deal"], a.btn')
                store_el = coupon.select_one('.store-name, .brand, img[alt]')
                
                discount = ""
                if discount_el:
                    discount = discount_el.get_text(strip=True)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Ø³Ø¨Ø© Ù…Ù† Ø£ÙŠ Ù…ÙƒØ§Ù†
                all_text = coupon.get_text()
                percent_match = re.search(r'(\d+)\s*%', all_text)
                if percent_match:
                    discount = f"{percent_match.group(1)}%"
                
                if discount and '%' in discount:
                    store = ""
                    if store_el:
                        store = store_el.get('alt', '') or store_el.get_text(strip=True)
                    
                    title = f"Ø®ØµÙ… {discount}"
                    if store:
                        title = f"Ø®ØµÙ… {discount} Ù…Ù† {store}"
                    
                    link = ""
                    if link_el:
                        link = link_el.get('href', '')
                    
                    if title:
                        offers.append({
                            'title': clean_title(title),
                            'link': link if link.startswith('http') else f"https://almowafir.com{link}",
                            'price': discount,
                            'category': 'Ø®ØµÙˆÙ…Ø§Øª',
                            'source': 'Ø§Ù„Ù…ÙˆÙØ±',
                            'date': datetime.now().isoformat()
                        })
            
            print(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(offers)} Ø¹Ø±Ø¶ Ù…Ù† Ø§Ù„Ù…ÙˆÙØ±")
    except Exception as e:
        print(f"Ø®Ø·Ø£ Ø§Ù„Ù…ÙˆÙØ±: {e}")
    return offers


def scrape_noon_deals():
    """Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ù†ÙˆÙ† Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©"""
    offers = []
    try:
        print("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† Ù†ÙˆÙ†...")
        url = "https://www.noon.com/saudi-ar/offers/"
        response = requests.get(url, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'
        })
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
            products = soup.select('[class*="product"], [class*="item"], article')[:15]
            
            for prod in products:
                title_el = prod.select_one('[class*="title"], [class*="name"], h3, h4')
                price_el = prod.select_one('[class*="price"], [class*="now"]')
                old_price = prod.select_one('[class*="was"], [class*="old"], del, s')
                link_el = prod.select_one('a[href]')
                
                if title_el and old_price:
                    title = clean_title(title_el.get_text(strip=True))
                    price = price_el.get_text(strip=True) if price_el else ""
                    link = link_el.get('href', '') if link_el else ""
                    
                    if title and len(title) > 5:
                        offers.append({
                            'title': f"Ø¹Ø±Ø¶ Ù†ÙˆÙ†: {title[:60]}",
                            'link': link if link.startswith('http') else f"https://noon.com{link}",
                            'price': price,
                            'category': 'ØªØ®ÙÙŠØ¶Ø§Øª',
                            'source': 'Ù†ÙˆÙ†',
                            'date': datetime.now().isoformat()
                        })
            
            print(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(offers)} Ù…Ù† Ù†ÙˆÙ†")
    except Exception as e:
        print(f"Ø®Ø·Ø£ Ù†ÙˆÙ†: {e}")
    return offers


def scrape_extra_deals():
    """Ø³Ø­Ø¨ Ø¹Ø±ÙˆØ¶ Ø§ÙƒØ³ØªØ±Ø§"""
    offers = []
    try:
        print("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† Ø§ÙƒØ³ØªØ±Ø§...")
        url = "https://www.extra.com/ar-sa/offers"
        response = requests.get(url, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 Chrome/120.0.0.0'
        })
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            products = soup.select('.product, .item, article, [class*="product"]')[:15]
            
            for prod in products:
                title_el = prod.select_one('.title, .name, h3, h4, a[title]')
                price_el = prod.select_one('.price, [class*="price"]')
                link_el = prod.select_one('a[href]')
                
                if title_el:
                    title = title_el.get('title') or title_el.get_text(strip=True)
                    title = clean_title(title)
                    price = price_el.get_text(strip=True) if price_el else ""
                    link = link_el.get('href', '') if link_el else ""
                    
                    if title and len(title) > 5:
                        offers.append({
                            'title': f"Ø¹Ø±Ø¶ Ø§ÙƒØ³ØªØ±Ø§: {title[:60]}",
                            'link': link if link.startswith('http') else f"https://extra.com{link}",
                            'price': price,
                            'category': 'Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ§Øª',
                            'source': 'Ø§ÙƒØ³ØªØ±Ø§',
                            'date': datetime.now().isoformat()
                        })
            
            print(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(offers)} Ù…Ù† Ø§ÙƒØ³ØªØ±Ø§")
    except Exception as e:
        print(f"Ø®Ø·Ø£ Ø§ÙƒØ³ØªØ±Ø§: {e}")
    return offers


def scrape_sample_offers():
    """Ø¹Ø±ÙˆØ¶ ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ù…Ù„ Ø§Ù„Ø¨ÙˆØª"""
    print("Ø¥Ø¶Ø§ÙØ© Ø¹Ø±ÙˆØ¶ ØªØ¬Ø±ÙŠØ¨ÙŠØ©...")
    return [
        {
            'title': 'Ø®ØµÙ… 30% Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø´Ø±ÙˆØ¨Ø§Øª Ù…Ù† Ø³ØªØ§Ø±Ø¨ÙƒØ³',
            'link': 'https://starbucks.sa/',
            'price': '30%',
            'category': 'Ù…Ø·Ø§Ø¹Ù…',
            'source': 'Ø³ØªØ§Ø±Ø¨ÙƒØ³',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'ÙƒØ§Ø´ Ø¨Ø§Ùƒ 15% Ù…Ø¹ Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠ Ø¹Ù„Ù‰ Ø£Ù…Ø§Ø²ÙˆÙ†',
            'link': 'https://amazon.sa/',
            'price': '15%',
            'category': 'Ø¨Ù†ÙˆÙƒ',
            'source': 'Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠ',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'Ø®ØµÙ… 50% Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬Ø¨Ø§Øª Ù…Ù† Ù‡Ù†Ù‚Ø±Ø³ØªÙŠØ´Ù†',
            'link': 'https://hungerstation.com/',
            'price': '50%',
            'category': 'ØªÙˆØµÙŠÙ„',
            'source': 'Ù‡Ù†Ù‚Ø±Ø³ØªÙŠØ´Ù†',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'ØªÙˆØµÙŠÙ„ Ù…Ø¬Ø§Ù†ÙŠ Ù…Ù† Ù†ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø·Ù„Ø¨Ø§Øª ÙÙˆÙ‚ 100 Ø±ÙŠØ§Ù„',
            'link': 'https://noon.com/',
            'price': 'Ù…Ø¬Ø§Ù†ÙŠ',
            'category': 'ØªØ³ÙˆÙ‚',
            'source': 'Ù†ÙˆÙ†',
            'date': datetime.now().isoformat()
        },
        {
            'title': 'Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ù…Ø¹Ø©: Ø®ØµÙ… 40% Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø²ÙŠØ§Ø¡ Ù…Ù† Ø´ÙŠ Ø§Ù†',
            'link': 'https://shein.com/',
            'price': '40%',
            'category': 'Ø£Ø²ÙŠØ§Ø¡',
            'source': 'Ø´ÙŠ Ø§Ù†',
            'date': datetime.now().isoformat()
        }
    ]


def clean_title(title: str) -> str:
    if not title:
        return ""
    title = re.sub(r'<[^>]+>', '', title)
    title = title.replace('*', '').replace('_', '').replace('[', '').replace(']', '')
    title = ' '.join(title.split())
    return title[:100] if title else ""


def extract_price(text: str) -> str:
    if not text:
        return ""
    patterns = [r'\d+%', r'\d+\s*(?:Ø±ÙŠØ§Ù„|Ø±\.Ø³|SAR)']
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group()
    return ""


def fetch_rss_offers(feed_url: str, feed_name: str, category: str):
    return []


def fetch_all_rss_feeds(feeds: list):
    """Ø³Ø­Ø¨ ÙƒÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶"""
    all_offers = []
    
    print("=" * 40)
    print("ğŸ” Ø¨Ø¯Ø¡ Ø³Ø­Ø¨ Ø§Ù„Ø¹Ø±ÙˆØ¶...")
    print("=" * 40)
    
    # Ø³Ø­Ø¨ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹
    try:
        all_offers.extend(scrape_almowafir_deals())
    except:
        pass
    
    try:
        all_offers.extend(scrape_noon_deals())
    except:
        pass
    
    try:
        all_offers.extend(scrape_extra_deals())
    except:
        pass
    
    # Ø¥Ø°Ø§ Ù…Ø§ ÙÙŠÙ‡ Ø¹Ø±ÙˆØ¶ØŒ Ù†Ø¶ÙŠÙ Ø¹Ø±ÙˆØ¶ ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    if len(all_offers) < 3:
        print("âš ï¸ Ø¹Ø±ÙˆØ¶ Ù‚Ù„ÙŠÙ„Ø©ØŒ Ø¥Ø¶Ø§ÙØ© Ø¹Ø±ÙˆØ¶ ØªØ¬Ø±ÙŠØ¨ÙŠØ©...")
        all_offers.extend(scrape_sample_offers())
    
    print("=" * 40)
    print(f"âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ø±ÙˆØ¶: {len(all_offers)}")
    print("=" * 40)
    
    return all_offers


def fetch_webpage_offers(url: str, selectors: dict):
    return []
