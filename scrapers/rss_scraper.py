import requests
from datetime import datetime
from bs4 import BeautifulSoup
import re
import json


def clean_text(text):
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', str(text))
    text = ' '.join(text.split())
    return text[:200]


def fetch_all_rss_feeds(feeds: list):
    """Ø³Ø­Ø¨ ÙƒÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©"""
    all_offers = []
    
    print("=" * 50)
    print("ğŸš€ Ø³Ø­Ø¨ Ø§Ù„ÙƒÙˆØ¨ÙˆÙ†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©...")
    print("=" * 50)
    
    # 1. ÙƒÙˆØ¨ÙˆÙ†Ø§Øª Ø§Ù„Ù…ÙˆÙØ±
    try:
        offers = scrape_almowafir()
        all_offers.extend(offers)
        print(f"âœ… Ø§Ù„Ù…ÙˆÙØ±: {len(offers)}")
    except Exception as e:
        print(f"âŒ Ø§Ù„Ù…ÙˆÙØ±: {e}")
    
    # 2. ÙƒÙˆØ¨ÙˆÙ† Ø³Ø¹ÙˆØ¯ÙŠ
    try:
        offers = scrape_couponsaudi()
        all_offers.extend(offers)
        print(f"âœ… ÙƒÙˆØ¨ÙˆÙ† Ø³Ø¹ÙˆØ¯ÙŠ: {len(offers)}")
    except Exception as e:
        print(f"âŒ ÙƒÙˆØ¨ÙˆÙ† Ø³Ø¹ÙˆØ¯ÙŠ: {e}")
    
    # 3. ÙƒÙˆØ¨ÙˆÙ† Ø¹Ø±Ø¨ÙŠ
    try:
        offers = scrape_couponarabi()
        all_offers.extend(offers)
        print(f"âœ… ÙƒÙˆØ¨ÙˆÙ† Ø¹Ø±Ø¨ÙŠ: {len(offers)}")
    except Exception as e:
        print(f"âŒ ÙƒÙˆØ¨ÙˆÙ† Ø¹Ø±Ø¨ÙŠ: {e}")
    
    print("=" * 50)
    print(f"âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(all_offers)}")
    
    return all_offers


def scrape_almowafir():
    """Ø³Ø­Ø¨ ÙƒÙˆØ¨ÙˆÙ†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù† Ø§Ù„Ù…ÙˆÙØ±"""
    offers = []
    
    # ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØ§Ø¬Ø± Ø§Ù„Ù…Ø´Ù‡ÙˆØ±Ø©
    stores = [
        ("noon", "Ù†ÙˆÙ†"),
        ("amazon-sa", "Ø£Ù…Ø§Ø²ÙˆÙ†"),
        ("shein", "Ø´ÙŠ Ø¥Ù†"),
        ("namshi", "Ù†Ù…Ø´ÙŠ"),
        ("hungerstation", "Ù‡Ù†Ù‚Ø±Ø³ØªÙŠØ´Ù†"),
        ("jahez", "Ø¬Ø§Ù‡Ø²"),
        ("talabat", "Ø·Ù„Ø¨Ø§Øª"),
        ("aliexpress", "Ø¹Ù„ÙŠ Ø§ÙƒØ³Ø¨Ø±Ø³"),
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0',
        'Accept-Language': 'ar-SA,ar;q=0.9',
        'Accept': 'text/html,application/xhtml+xml'
    }
    
    for slug, name in stores:
        try:
            url = f"https://almowafir.com/ar/stores/{slug}/"
            resp = requests.get(url, headers=headers, timeout=15)
            
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙˆØ¨ÙˆÙ†Ø§Øª
                # Ø§Ù„Ù…ÙˆÙØ± ÙŠØ³ØªØ®Ø¯Ù… data attributes Ù„Ù„ÙƒÙˆØ¯Ø§Øª
                coupons = soup.find_all(['div', 'section'], class_=lambda x: x and ('coupon' in x.lower() or 'offer' in x.lower()))
                
                for coupon in coupons[:3]:
                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆØ¯
                    code = None
                    
                    # 1. Ù…Ù† data attribute
                    code = coupon.get('data-code') or coupon.get('data-coupon')
                    
                    # 2. Ù…Ù† Ø¹Ù†ØµØ± Ø¯Ø§Ø®Ù„ÙŠ
                    if not code:
                        code_el = coupon.find(class_=lambda x: x and 'code' in x.lower())
                        if code_el:
                            code = code_el.get_text(strip=True)
                    
                    # 3. Ù…Ù† input
                    if not code:
                        code_input = coupon.find('input', {'type': 'text'})
                        if code_input:
                            code = code_input.get('value')
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØµÙ
                    desc_el = coupon.find(['h3', 'h4', 'p', 'span'], class_=lambda x: x and ('title' in str(x).lower() or 'desc' in str(x).lower()))
                    desc = desc_el.get_text(strip=True) if desc_el else ""
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø³Ø¨Ø© Ø§Ù„Ø®ØµÙ…
                    text = coupon.get_text()
                    percent = re.search(r'(\d+)\s*%', text)
                    discount = f"{percent.group(1)}%" if percent else "Ø®ØµÙ…"
                    
                    if code or desc:
                        offers.append({
                            'title': f"ÙƒÙˆØ¨ÙˆÙ† {name}: {clean_text(desc)[:50]}" if desc else f"ÙƒÙˆØ¨ÙˆÙ† {name}",
                            'link': url,
                            'price': code if code else discount,
                            'category': 'ÙƒÙˆØ¨ÙˆÙ†Ø§Øª',
                            'source': name,
                            'image_url': '',
                            'description': f"""ğŸ« *ÙƒÙˆØ¨ÙˆÙ† {name}*

ğŸ’° Ø§Ù„ÙƒÙˆØ¯: *{code if code else 'Ø§Ø¶ØºØ· Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯'}*
ğŸ“Š Ø§Ù„Ø®ØµÙ…: {discount}

âœ… Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
1. Ø§Ù†Ø³Ø® Ø§Ù„ÙƒÙˆØ¯
2. Ø§Ø°Ù‡Ø¨ Ù„Ù„Ù…ÙˆÙ‚Ø¹
3. Ø§Ù„ØµÙ‚ Ø§Ù„ÙƒÙˆØ¯ Ø¹Ù†Ø¯ Ø§Ù„Ø¯ÙØ¹

ğŸ”— Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹: {url}""",
                            'date': datetime.now().isoformat()
                        })
                        
        except Exception as e:
            print(f"  Ø®Ø·Ø£ {name}: {e}")
            continue
    
    return offers


def scrape_couponsaudi():
    """Ø³Ø­Ø¨ Ù…Ù† Ù…ÙˆÙ‚Ø¹ ÙƒÙˆØ¨ÙˆÙ† Ø³Ø¹ÙˆØ¯ÙŠ"""
    offers = []
    
    try:
        url = "https://www.couponsaudi.com/"
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=15)
        
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ÙƒÙˆØ¨ÙˆÙ†Ø§Øª
            cards = soup.find_all(['div', 'article'], class_=lambda x: x and any(k in str(x).lower() for k in ['coupon', 'deal', 'offer', 'card']))
            
            for card in cards[:10]:
                title = card.find(['h2', 'h3', 'h4'])
                link = card.find('a')
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙˆØ¯
                code_el = card.find(class_=lambda x: x and 'code' in str(x).lower())
                code = code_el.get_text(strip=True) if code_el else None
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø®ØµÙ…
                text = card.get_text()
                percent = re.search(r'(\d+)\s*%', text)
                
                if title:
                    title_text = clean_text(title.get_text())
                    offers.append({
                        'title': title_text,
                        'link': link.get('href', url) if link else url,
                        'price': code if code else (f"{percent.group(1)}%" if percent else "Ø®ØµÙ…"),
                        'category': 'ÙƒÙˆØ¨ÙˆÙ†Ø§Øª',
                        'source': 'ÙƒÙˆØ¨ÙˆÙ† Ø³Ø¹ÙˆØ¯ÙŠ',
                        'image_url': '',
                        'description': f"ğŸ« {title_text}\n\n{'ğŸ“‹ Ø§Ù„ÙƒÙˆØ¯: ' + code if code else ''}\n\nâœ… ÙƒÙˆØ¨ÙˆÙ† ÙØ¹Ø§Ù„ Ù…Ù† ÙƒÙˆØ¨ÙˆÙ† Ø³Ø¹ÙˆØ¯ÙŠ",
                        'date': datetime.now().isoformat()
                    })
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙƒÙˆØ¨ÙˆÙ† Ø³Ø¹ÙˆØ¯ÙŠ: {e}")
    
    return offers


def scrape_couponarabi():
    """Ø³Ø­Ø¨ Ù…Ù† Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ÙƒÙˆØ¨ÙˆÙ†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    offers = []
    
    sites = [
        "https://www.coupon.ae/ar/",
        "https://www.alcoupon.com/ar/",
    ]
    
    headers = {'User-Agent': 'Mozilla/5.0', 'Accept-Language': 'ar'}
    
    for site_url in sites:
        try:
            resp = requests.get(site_url, headers=headers, timeout=15)
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª
                cards = soup.find_all(['div', 'article'], limit=20)
                
                for card in cards:
                    # ÙÙ„ØªØ±Ø© Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
                    text = card.get_text().lower()
                    if not any(k in text for k in ['Ø®ØµÙ…', 'ÙƒÙˆØ¨ÙˆÙ†', 'ÙƒÙˆØ¯', '%', 'offer', 'discount']):
                        continue
                    
                    title = card.find(['h2', 'h3', 'h4', 'a'])
                    if not title:
                        continue
                        
                    title_text = clean_text(title.get_text())
                    if len(title_text) < 5:
                        continue
                    
                    # Ø§Ù„Ø®ØµÙ…
                    percent = re.search(r'(\d+)\s*%', card.get_text())
                    
                    # Ø§Ù„ÙƒÙˆØ¯
                    code = None
                    code_el = card.find(attrs={'data-clipboard-text': True})
                    if code_el:
                        code = code_el.get('data-clipboard-text')
                    
                    link = card.find('a')
                    
                    offers.append({
                        'title': title_text[:60],
                        'link': link.get('href', site_url) if link else site_url,
                        'price': code if code else (f"{percent.group(1)}%" if percent else "Ø®ØµÙ…"),
                        'category': 'ÙƒÙˆØ¨ÙˆÙ†Ø§Øª',
                        'source': 'ÙƒÙˆØ¨ÙˆÙ† Ø¹Ø±Ø¨ÙŠ',
                        'image_url': '',
                        'description': f"ğŸ« {title_text}\n\nâœ… ÙƒÙˆØ¨ÙˆÙ† ÙØ¹Ø§Ù„",
                        'date': datetime.now().isoformat()
                    })
                    
                    if len(offers) >= 5:
                        break
                        
        except Exception as e:
            print(f"Ø®Ø·Ø£ {site_url}: {e}")
            continue
    
    return offers


def fetch_rss_offers(feed_url: str, feed_name: str, category: str):
    return []

def fetch_webpage_offers(url: str, selectors: dict):
    return []
